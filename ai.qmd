---
title: "AI-based protein modeling with Colabfold"
author: "Modesto Redrejo Rodr√≠guez"
date: "`r Sys.Date()`"
toc: true 
toc_float: true
format:
  html:
    theme: simplex
    toc: true
    toc-location: left
    toc-depth: 4
    number-sections: false
    code-overflow: wrap
    link-external-icon: true
    link-external-newwindow: true
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: 80
---

# How did we get to the Era of Alphafold (& rel.)?

As mentioned earlier, the introduction of HMM-based profiles during the first
decade of this century led to a great improvement in template detection and
protein modeling in the twilight-zone, i.e., proteins with only distant homologs
(\<25-30% identity) in databases. These methods naturally evolved to iterative
[threading]{.ul} methods, based on multitemplate model construction, implemented
in [I-TASSER](https://zhanggroup.org/I-TASSER/) [@roy2010],
[RaptorX](http://raptorx.uchicago.edu/) [@peng2011] and
[HHpred](https://toolkit.tuebingen.mpg.de/tools/hhpred) [@meier2015], among
others.

![Contact-based map of representative proteins. The map represents a matrix of
amino acid positions in the protein sequences (on both, the X and Y axis); with
contacts indicated as blue dots. When a number of consecutive residues in the
sequence interact the dots form diagonal stretches. Maps obtained at
<http://cmweb.enzim.hu/>](pics/contact.png "Contact-based map")

During the last decade, the introduction of residue-residue contact or distance
maps prediction based on sequence co-evolution and *deep learning* started a
revolution in the field that crystallize with the arrival of Alphafold2 and
RoseTTAfold as major breakthroughs with great repercussions in diverse fields.

![Schematic of how co-evolution methods extract information about protein
structure from a multiple sequence alignment (MSA). Image modified from doi:
`10.5281/zenodo.1405369`, which in turn was modified from doi:
`10.1371/journal.pone.0028766`](pics/coevolution.webp)

As shown in the picture below, residue contact maps are a 2D matrix-like
representation of the protein sequence in which each pair of interacting
residues are indicated. An accurate information of protein's residue--residue
contacts is sufficient to elucidate the fold of a protein @olmea1997, however
obtaining this map is not always easy. The introduction of **evolutionary
coupling analysis (ECA)**, i.e., extract the residue coevolution from MSAs
(piture above) improved contact maps and allowed their implementation for
protein folding in several methods, like PSICOV [@jones2012] or Gremlin
[@kamisetty2013], among others. However, for proteins without many sequence
homologs, the predicted contacts were of low quality and insufficient for
accurate contact-assisted protein modeling.

![Illustration of column pair and precision submatrix grouping for advanced
prediction of contact maps. In the example, Columns 5 and 14 in the first family
are aligned to columns 5 and 11 in the second family, respectively, so column
pair (5,14) in the first family and the pair (5,11) in the second family are
assigned to the same group. Accordingly, the two precision submatrices will be
asigned to the same group. From @ma2015.](pics/contact2.gif)

Deep learning is a sub-field of machine learning which is based on artificial
neural networks. Neural networks were introduced actually in the late 40's and
50's, but they reappeared in the 2000's thanks to the increase of computational
capacities and the use of
[GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit). Briefly, a NN
uses multiple interconnected layers to transform multiple inputs (MSAs,
high-resolution contact based maps...) into compound features that can be used
to predict a complex output, like a 3D protein structure. As their name
indicate, NNs attempt to simulate the behavior of the human brain that process
large amounts of data and can be trained to "learn" from that data. Deep
learning is based in the use of multiple layer-NN to optimize and refine for
accuracy.

In this context, introduction of supervised machine learning methods that
predict contacts from distant protein families, outperforming ECA methods by the
use of multilayer neural networks [@jones2015, @ma2015]. This methods also
benefit from the use of the so-called high resolution contact maps, that
contained not only contact information, but also probabilities, distances and
angles.

![Example of high-resolution contact maps of 6MSP. From
@yang2020](pics/high_res_maps.png "High-resolution contact maps")

## The recent history of protein structure modeling telling by a contest (CASP)

Every two years since 1994, structural bioinformatics groups carry out a
worldwide experiment, predicting a set of unknown protein structures in a
controlled competition and comparing their output with the experimentally
obtained structure. This is the **CASP** or [*Critical assessment of Protein
Structure Prediction*](https://predictioncenter.org/).

[![Comparative z-core of CASP13 participants. The score is based in the GDT_TS
(Global distance
test).](pics/casp13.png "CASP13 results")](https://predictioncenter.org/casp13/zscores_final.cgi?formula=gdt_ts)

The best research groups in the field test their new methods and protocols in
CASP. However, in CASP13 (2018) an AI company called
[Deepmind](https://en.wikipedia.org/wiki/DeepMind) (Google Subsidiary) entered
in the scene. Their method, named Alphafold [@senior2020] clearly won CASP13.
Alphafold implemented some improvements in a few recently used approaches,
creating a new whole pipeline. Basically, instead of create contact maps from
the alignment to then fold the structure, they used a MRF unit (Markov Random
Field) to extract in advance the main features of sequence and the MSA and
process all of that info into a multilayer NN (called ResNet) that provides the
distant map and other information. Then, Alphafold uses all the possibly
obtained information to create the structure and then improve it by energy
minimization and substitution of portions with a selected DB of protein
fragments.

[![Workflow of the first Alphafold method presented in CASP13. MSA stands for
multiple sequence alignment; PSSM indicates Position-specific-scoring matrix and
MRF stands for Markov Random Field (or Potts model). From the Sergei Ovchinnikov
and Martin Steinegger presentation of Alphafold2 to the Boston Protein Design
Group (link
below)](pics/alphafold1.png "Alphafold1")](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI)

After Alphafold, similar methods were also developed and made available to the
general public, like the *trRosetta* from Baker lab [@yang2020], available in
the [Robetta](https://robetta.bakerlab.org/) server. This led to some
controversy (mostly on Twitter) about the open access to the CASP software and
later on DeepMind publishes all the code on GitHub.

# Alphafold2 started a new era

In CASP14 the expectation was very high and the guys from DeepMind did not
disappoint anyone. Alphafold2 highly outperformed all competitors, both in
relative (score respect the other groups) and absolute terms (lowest alpha-C
RMSD).

[![Comparative CASP14
scores](pics/casp14.png "CASP14 scores")](https://predictioncenter.org/casp14/zscores_final.cgi?formula=gdt_ts)

[![Performance of Alphafold2 the CASP14 dataset relative to the top-15 entries.
Data are median and the 95% confidence interval of the median, for alpha-carbom
RMSD. Panels b-c-d show example comparison between model and experimental
structures.](pics/jumper2021.png "Alphafold2")](https://www.nature.com/articles/s41586-021-03819-2/figures/1)

Alphafold took some time to publish the method \[@jumper2021\] and make it
available on [Github](https://github.com/deepmind/alphafold), but other new
methods, like RoseTTAfold \[@baek2021\] and C-I-Tasser \[@zheng2021\] were
available on public servers from the very beginning. Surprisingly (at least for
me) a group of independent scientists ([Sergey
Ovchinnikov](https://twitter.com/sokrypton), [Milot
Mirdita](https://twitter.com/milot_mirdita), and [Martin
Steinegger](https://twitter.com/thesteinegger)), decided to implement Alphafold2
in a [Colab notebook](https://github.com/sokrypton/ColabFold), named ColabFold
@mirdita2022, freely available online. They implemented some tricks to
accelerate the modeling, mainly the use of
[MMSeqs2](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI/present#slide=id.ge58c80b745_0_15)
(developed by Martin Steinegger's group) to search for homolog structures on
Uniref30, which made Colabfold a quick method that made all the previous
advanced methods almost useless. This was the real breakthrough in the protein
structure field, making Alphafold2 available to every one and, also very
important, facilitate the evolution of the method, implementing new features,
like the prediction of protein complexes \[@evans\], which was actually
mentioned first on
[Twitter](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI/present#slide=id.ge58c80b745_0_15)
(slide 21)!

+----------------+----------------+--------------------------------------------+
| Method         | Reference(s)   | Link                                       |
+================+================+============================================+
| RaptorX        | @wang2017      | <http://raptorx.uchicago.edu/ContactMap/>  |
| -contact       |                |                                            |
+----------------+----------------+--------------------------------------------+
| CE-threader    | @zheng2019     | <https://zhanggroup.org/CEthreader/>       |
+----------------+----------------+--------------------------------------------+
| DisCovEr       | @bh            | <htt                                       |
|                | attacharya2022 | ps://github.com/Bhattacharya-Lab/DisCovER> |
+----------------+----------------+--------------------------------------------+
| trRosetta      | @yang2020      | <https://robetta.bakerlab.org/>            |
+----------------+----------------+--------------------------------------------+
| Alphafold      | @senior2020    | \<https://github.com/deepm                 |
|                |                | ind/deepmin                                |
|                |                | d-research/tree/master/alphafold_casp13aa> |
+----------------+----------------+--------------------------------------------+
| RoseTTAfold    | @baek2021      | <https://robetta.bakerlab.org/>            |
+----------------+----------------+--------------------------------------------+
| Alphafold2     | @jumper2021\   | [https://git                               |
|                | & @mirdita2022 | hub.com/deepmind/alphafol                  |
|                |                | d](https://githu%20b.com/dee%20pmind/alpha |
|                |                | foldhttps://github.com/deepmind/alphafold) |
|                |                |                                            |
|                |                | <https://github.com/sokrypton/ColabFold>   |
+----------------+----------------+--------------------------------------------+
| C-I-Tasser     | @zheng2021     | <https://zhanggroup.org/C-I-TASSER/>       |
+----------------+----------------+--------------------------------------------+

: Protein modeling methods based on inter-residue interaction map threading
using deep learning

## Why is Alphafold2 so freaking accurate?

The philosophy behind Alphafold and Alphafold2 is treating the protein folding
problem as a machine learning problem of image processing. In all these
problems, the input to the Deep Learning model is a volume (3D tensor). In the
case of computer vision, 2D images expand as a volume because of the RGB or HSV
channels. Similarly, in the case of distance prediction, predicted 1D and 2D
features are transformed and packed into 3D volume with many channels of
inter-residue information [@pakhrin2021].

![From the perspective of Deep Learning method development, the problem of
protein distogram or real-valued distance prediction (bottom row) is similar to
the 'depth prediction problem' in computer vision. From](pics/machine_fold.png)

Alphafold2 can be explained in three independent tasks (see picture below).
First, it queries several databases of protein sequences and constructs an MSA
that is used to select templates. In the second part of the diagram, AlphaFold 2
takes the multiple sequence alignment and the templates, and processes them in a
*transformer*. The objective of this part is to extract layers of information to
generate residue interaction maps. A better model of the MSA will improve the
network's characterization of the geometry, which simultaneously will help
refine the model of the MSA. Importantly, this process is iterative and the
number of recycling steps improves the model (the original model uses 48 blocks
of cycles).

The third part of the pipeline is the structure building module, which uses the
information from the previous steps to construct a 3D model structure protein of
the query sequence. This network will give you a single model, without any
energy optimization step.

![Oxford Proteins Informatics Group Blog, modified
From](pics/alphafols2.png "Alphafold2")

As mentioned above, Colabfold aims to make the process faster by using MMSeqs in
the first block. Additionally, the number of recycling steps can also be
adapted. Moreover, different Colabfold notebooks have been developed (and
evolved) to allow some customization and other feature, like batch processing of
multiple proteins and identification of protein-protein interactions
[@mirdita2022].

Alphafold models can be evaluated by the mean **pLDDT**, a per-residue
confidence metric. The model confidence can vary greatly along a chain so it is
important to consult the confidence when interpreting structural features. Very
often, the lower confidence fragments are not product of a poor prediction but
an indicator of protein disorder.

## Let's try Alphafold2.

Section under construction!

## Corollary: Has Levinthal's paradox "folded"?

The development of Alphafold and the [Alphafold structures
Database](https://alphafold.ebi.ac.uk/) in collaboration with
[EMBL-EBI](https://www.ebi.ac.uk/about) has been the origin of a New Era.
Scientific publications and journals worldwide published long articles about the
meaning of this breakthrough in science and its applications in biotechnology
and biomedicine[^1] and DeepMind claimed to have [Solved a 50-years Grand
Challenge in
biochemistry](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology).
The coverage of the protein structure space has been greatly increased
[@porta-pardo2022].

[^1]: https://www.bbc.com/news/science-environment-57929095

    https://www.forbes.com/sites/robtoews/2021/10/03/alphafold-is-the-most-important-achievement-in-ai-ever/

    https://elpais.com/ciencia/2021-07-22/la-forma-de-los-ladrillos-basicos-de-la-vida-abre-una-nueva-era-en-la-ciencia.html

However, some scientists have claimed that Alphafold2 and RoseTTAfold actually
"cheat" a bit as it does not really solve the problem but generate a deep
learning pipeline that "bypass" the problem [@pederson2021]. In agreement with
that, it has been shown that machine learning methods actually do not reproduce
the expected folding pathways while improving the structures during the
recycling steps @outeiral.

In conclusion, I do believe that Levinthal's paradox has not been (yet) fully
solved, but almost, and solving it will probably reduce the limitations of
Alphafold2. However, [CASP15](https://predictioncenter.org/casp15/index.cgi) is
currently being held and maybe I will have to change my mind later this year.

# Useful links

-   Introductory article to Neural Networks at the IBM site:
    <https://www.ibm.com/cloud/learn/neural-networks>

-   ColabFold Tutorial presented presented by Sergey Ovchinnikov and Martin
    Steineggerat the Boston Protein Design and Modeling Club (6 ago 2021).
    [\[video\]](https://www.youtube.com/watch?v=Rfw7thgGTwI)
    [\[slides\]](https://docs.google.com/presentation/d/1mnffk23ev2QMDzGZ5w1skXEadTe54l8-Uei6ACce8eI).

-   Post about Alphafold2 in the Oxford Protein Informatics Group site:
    <https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/>

-   A very good digest article about the Alphafold2 paper:
    <https://moalquraishi.wordpress.com/2021/07/25/the-alphafold2-method-paper-a-fount-of-good-ideas/>

-   A post that explain how Alphafold2 and RoseTTAfold code became publically
    available:
    <https://www.wired.com/story/without-code-for-deepminds-protein-ai-this-lab-wrote-its-own/>

# References
